{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Text Classification\n",
    "## 1.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import some useful packages to do the task\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>ReviewTitle</th>\n",
       "      <th>ReviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Incorrect Disc</td>\n",
       "      <td>I love the style of this, but after a couple y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD menu select problems</td>\n",
       "      <td>I cannot scroll through a DVD menu that is set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Unique Weird Orientalia from the 1930's</td>\n",
       "      <td>Exotic tales of the Orient from the 1930's. \"D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Not an \"ultimate guide\"</td>\n",
       "      <td>Firstly,I enjoyed the format and tone of the b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                        ReviewTitle  \\\n",
       "0      2                                           Great CD   \n",
       "1      2  One of the best game music soundtracks - for a...   \n",
       "2      1                   Batteries died within a year ...   \n",
       "3      2              works fine, but Maha Energy is better   \n",
       "4      2                       Great for the non-audiophile   \n",
       "5      1              DVD Player crapped out after one year   \n",
       "6      1                                     Incorrect Disc   \n",
       "7      1                           DVD menu select problems   \n",
       "8      2            Unique Weird Orientalia from the 1930's   \n",
       "9      1                            Not an \"ultimate guide\"   \n",
       "\n",
       "                                          ReviewText  \n",
       "0  My lovely Pat has one of the GREAT voices of h...  \n",
       "1  Despite the fact that I have only played a sma...  \n",
       "2  I bought this charger in Jul 2003 and it worke...  \n",
       "3  Check out Maha Energy's website. Their Powerex...  \n",
       "4  Reviewed quite a bit of the combo players and ...  \n",
       "5  I also began having the incorrect disc problem...  \n",
       "6  I love the style of this, but after a couple y...  \n",
       "7  I cannot scroll through a DVD menu that is set...  \n",
       "8  Exotic tales of the Orient from the 1930's. \"D...  \n",
       "9  Firstly,I enjoyed the format and tone of the b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the text data file as the training data set\n",
    "TrainDF = pd.read_csv('textdata.csv', encoding ='UTF-8')\n",
    "\n",
    "# After reading the file have a look at it\n",
    "TrainDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the review title and review text both contains some useful information. But in order to simplify the work we can combine them togerther as a new attribute \"Review\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Great CD : My lovely Pat has one of the GREAT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ... : I bought th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better : Check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile : Reviewed quite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year : I also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Incorrect Disc : I love the style of this, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD menu select problems : I cannot scroll thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Unique Weird Orientalia from the 1930's : Exot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Not an \"ultimate guide\" : Firstly,I enjoyed th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Review\n",
       "0      2  Great CD : My lovely Pat has one of the GREAT ...\n",
       "1      2  One of the best game music soundtracks - for a...\n",
       "2      1  Batteries died within a year ... : I bought th...\n",
       "3      2  works fine, but Maha Energy is better : Check ...\n",
       "4      2  Great for the non-audiophile : Reviewed quite ...\n",
       "5      1  DVD Player crapped out after one year : I also...\n",
       "6      1  Incorrect Disc : I love the style of this, but...\n",
       "7      1  DVD menu select problems : I cannot scroll thr...\n",
       "8      2  Unique Weird Orientalia from the 1930's : Exot...\n",
       "9      1  Not an \"ultimate guide\" : Firstly,I enjoyed th..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the \"ReviewTitle\" with \"ReviewText\"\n",
    "TrainDF[\"Review\"] = TrainDF[\"ReviewTitle\"] + \" : \" + TrainDF[\"ReviewText\"]\n",
    "\n",
    "# Then drop the original two columns\n",
    "TrainDF = TrainDF.drop([\"ReviewTitle\",\"ReviewText\"],1)\n",
    "\n",
    "# Check it is right or not\n",
    "TrainDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split the current dataset into 2 parts. One as the training set, another one as the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the data set splitting through model_selection method\n",
    "trainX, testX, trainY, testY = model_selection.train_test_split(TrainDF[\"Review\"],TrainDF[\"Class\"])\n",
    "\n",
    "# Set the \"Class\" as the target variables\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "trainY = encoder.fit_transform(trainY)\n",
    "testY = encoder.fit_transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Text Preprocessing(Demo)\n",
    "### 1.3.1 Noise Removal\n",
    "In these part, we can remove some noise to make our task more easy to accomplish. There are mainly 2 kinds of noise:\n",
    "1. Noisy word: The words that are not relevant to the context of the data\n",
    "2. Special pattern: Some characters.\n",
    "\n",
    "#### Function to remove the noisy word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the noisy words\n",
    "\n",
    "# Prepare a dictionary of noisy entities \n",
    "noiseList = [\"this\", \"so\", \"is\", \"very\", \"really\", \"he\", \"she\", \"i\", \"it\", \"the\", \"are\", \"them\"]\n",
    "\n",
    "# Function definition\n",
    "def remove_noisy_word(text):\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove the noisy words\n",
    "    usefulwords = [word for word in words if word.lower() not in noiseList]\n",
    "\n",
    "    # Combine the useful words together in order to make a new clean text\n",
    "    cleantext = \" \".join(usefulwords)\n",
    "    \n",
    "    # Return the clean text\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to remove the special pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove all the special patterns\n",
    "\n",
    "# Function definition\n",
    "def remove_pattern(text):\n",
    "    \n",
    "    # Remove the special characters:\n",
    "    cleantext = \"\".join([char for char in text if char in string.ascii_letters or char in string.whitespace])\n",
    "    \n",
    "    # Return the clean text\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Small Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Mining course interesting teachers nice love much'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sentence that we obtained from the social network\n",
    "\n",
    "sentence = \"#Data Mining : This course is really, really, really interesting. The teachers are very, very, very nice! I love them so so so so so so much!\"\n",
    "\n",
    "remove_noisy_word(remove_pattern(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Lexicon Normalization\n",
    "In fact, there is another type of textual noise. That is about the multiple representations exhibited by single word.\n",
    "\n",
    "There are mainly two ways to solve this problem:\n",
    "1. Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "2. Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "#### A Small Demo Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "multipli\n"
     ]
    }
   ],
   "source": [
    "# New a lemmatizer and a porter stemmer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\"\n",
    "\n",
    "print(lem.lemmatize(word,\"v\"))\n",
    "print(stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 Object Standardization\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models. So, we also need to deal with them:\n",
    "\n",
    "### Function to do the Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do the standardization for all the acronyms etc.\n",
    "\n",
    "# Prepare a dictionary of all the acronyms\n",
    "lookupDict = {'xswl':'It is really funny', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "\n",
    "# Function Definition\n",
    "def lookupWords(text):\n",
    "    \n",
    "    words = text.split() \n",
    "    \n",
    "    # A new tuple to store the changed words\n",
    "    newwords = [] \n",
    "    \n",
    "    # Use a loop to check every word\n",
    "    for word in words:\n",
    "        \n",
    "        # Change the word if it is an acronym\n",
    "        if word.lower() in lookupDict:\n",
    "            word = lookupDict[word.lower()]\n",
    "            \n",
    "        # Put the changed word back to it\n",
    "        newwords.append(word) \n",
    "        \n",
    "    # Combine all the words together\n",
    "    stdtext = \" \".join(newwords) \n",
    "        \n",
    "    # Return the standardization text \n",
    "    return stdtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Small Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is really funny The moive is awesome I love it\n",
      "funny moive awesome love\n"
     ]
    }
   ],
   "source": [
    "# A sentence that we obtained from the social network\n",
    "\n",
    "sentence = \"xswl! The moive is awsm. I luv it!\"\n",
    "\n",
    "print(lookupWords(remove_pattern(sentence)))\n",
    "print(remove_noisy_word(lookupWords(remove_pattern(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4 Other Technology\n",
    "Some other technologies for text preprocessing:\n",
    "1. Spelling Correction\n",
    "2. Grammer Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Feature Engineering\n",
    "\n",
    "### 1.4.1 Use Count Vector as Feature Vector\n",
    "\n",
    "Count vector is the matrix representation of the data set. For a(ij) in the matrix, it is the word frequency of j in the text i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector counter object\n",
    "countVect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "countVect.fit(TrainDF['Review'])\n",
    "\n",
    "# Use the vecter counter object to transform the text data\n",
    "xtraincount = countVect.transform(trainX)\n",
    "xtestcount = countVect.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Use TF-IDF Vector as Feature Vector\n",
    "We can use statistical features to represent our data set. According to this idea, Term Frequency–Inverse Document Frequency(TF–IDF) is a good measurement for feature engineering.\n",
    "\n",
    "There are mainly two kinds of TF-IDF vectors:\n",
    "1. Word-Level TF-IDF\n",
    "2. N-gram TF-IDF\n",
    "\n",
    "#### Word-Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Word-Level TF-IDF vectors\n",
    "tfidfVect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000)\n",
    "tfidfVect.fit(TrainDF['Review'])\n",
    "\n",
    "xtrainTFIDF = tfidfVect.transform(trainX)\n",
    "xtestTFIDF = tfidfVect.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the n-gram TF-IDF vectors (n = 2,3)\n",
    "tfidfVectngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=1000)\n",
    "tfidfVectngram.fit(TrainDF['Review'])\n",
    "\n",
    "xtrainTFIDFngram = tfidfVectngram.transform(trainX)\n",
    "xtestTFIDFngram = tfidfVectngram.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Use Text Vector as Feature Vector (Word Embedding)\n",
    "### 1.4.4 Use Topic Modeling as Feature Vector\n",
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "### 1.4.5 Use Other Vectors as Feature Vector\n",
    "Sometimes, according to some specific situation, we can use some other vectors as the feature vectors. For example, the number of total words, the number of different part of speech words, the average length of words. It may improve our model case-by-case.\n",
    "## 1.5 Modeling\n",
    "### 1.5.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that can do the training and testing of the modeling \n",
    "\n",
    "def trainModel(classifier, feature_vector_train, label, feature_vector_test, is_neural_net=False):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "\n",
    "    # If current model is neural network, we need do the special operation of it\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    # Return its accuracy score\n",
    "    return metrics.accuracy_score(predictions, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Naive Bayes\n",
    "A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors: 0.812\n",
      "Naive Bayes, WordLevel TF-IDF: 0.796\n",
      "Naive Bayes, N-Gram Vectors: 0.742\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with count vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtraincount, trainY, xtestcount)\n",
    "print (\"Naive Bayes, Count Vectors: {0}\".format(accuracy))\n",
    "\n",
    "# Naive Bayes with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"Naive Bayes, WordLevel TF-IDF: {0}\".format(accuracy))\n",
    "\n",
    "# Naive Bayes with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"Naive Bayes, N-Gram Vectors: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Reason for the Poor Performance of  N-gram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors: 0.812\n",
      "Naive Bayes, WordLevel TF-IDF: 0.796\n",
      "Naive Bayes, N-Gram Vectors: 0.796\n"
     ]
    }
   ],
   "source": [
    "# Change the max_features\n",
    "tfidfVectngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=12500)\n",
    "tfidfVectngram.fit(TrainDF['Review'])\n",
    "\n",
    "xtrainTFIDFngram = tfidfVectngram.transform(trainX)\n",
    "xtestTFIDFngram = tfidfVectngram.transform(testX)\n",
    "\n",
    "# Naive Bayes with count vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtraincount, trainY, xtestcount)\n",
    "print (\"Naive Bayes, Count Vectors: {0}\".format(accuracy))\n",
    "\n",
    "# Naive Bayes with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"Naive Bayes, WordLevel TF-IDF: {0}\".format(accuracy))\n",
    "\n",
    "# Naive Bayes with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(naive_bayes.MultinomialNB(), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"Naive Bayes, N-Gram Vectors: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Logistic Regression\n",
    "Logistic Regression is a linear classifier that use logistic / sigmoid function to esitimate the probability in order to do the classfication task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37121\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, Count Vectors:  0.814\n",
      "Logistic Regression, WordLevel TF-IDF:  0.81\n",
      "Logistic Regression, N-Gram Vectors:  0.784\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = trainModel(linear_model.LogisticRegression(), xtraincount, trainY, xtestcount)\n",
    "print (\"Logistic Regression, Count Vectors:  {0}\".format(accuracy))\n",
    "\n",
    "# Linear Classifier on WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(linear_model.LogisticRegression(), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"Logistic Regression, WordLevel TF-IDF:  {0}\".format(accuracy))\n",
    "\n",
    "# Linear Classifier on N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(linear_model.LogisticRegression(), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"Logistic Regression, N-Gram Vectors:  {0}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.4 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37121\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM , Count Vectors:  0.52\n",
      "SVM , WordLevel TF-IDF:  0.52\n",
      "SVM , N-Gram Vectors:  0.52\n"
     ]
    }
   ],
   "source": [
    "# SVM with Count Vectors\n",
    "accuracy = trainModel(svm.SVC(), xtraincount, trainY, xtestcount)\n",
    "print (\"SVM , Count Vectors:  {0}\".format(accuracy))\n",
    "\n",
    "# SVM with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(svm.SVC(), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"SVM , WordLevel TF-IDF:  {0}\".format(accuracy))\n",
    "\n",
    "# SVM with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(svm.SVC(), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"SVM , N-Gram Vectors:  {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the data is not linear sperable. So, SVM may not be considered as a good model to do the text classification\n",
    "### 1.5.5 Random Forest (Bagging Model)\n",
    "Random forest is like bootstrapping algorithm with Decision tree (CART) model. It tries to build multiple CART models with different samples and different initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest , Count Vectors:  0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37121\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\37121\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest , WordLevel TF-IDF:  0.734\n",
      "Random Forest , N-Gram Vectors:  0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37121\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with Count Vectors\n",
    "accuracy = trainModel(ensemble.RandomForestClassifier(), xtraincount, trainY, xtestcount)\n",
    "print (\"Random Forest , Count Vectors:  {0}\".format(accuracy))\n",
    "\n",
    "# Random Forest with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(ensemble.RandomForestClassifier(), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"Random Forest , WordLevel TF-IDF:  {0}\".format(accuracy))\n",
    "\n",
    "# Random Forest with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(ensemble.RandomForestClassifier(), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"Random Forest , N-Gram Vectors:  {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6 Xgboost (Boosting Model)\n",
    "Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine.\n",
    "\n",
    "This makes xgboost at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost , Count Vectors:  0.75\n",
      "Xgboost , WordLevel TF-IDF:  0.754\n",
      "Xgboost , N-Gram Vectors:  0.678\n"
     ]
    }
   ],
   "source": [
    "# Xgboost with Count Vectors\n",
    "accuracy = trainModel(xgboost.XGBClassifier(), xtraincount.tocsc(), trainY, xtestcount.tocsc())\n",
    "print (\"Xgboost , Count Vectors:  {0}\".format(accuracy))\n",
    "\n",
    "# Xgboost with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(xgboost.XGBClassifier(), xtrainTFIDF.tocsc(), trainY, xtestTFIDF.tocsc())\n",
    "print (\"Xgboost , WordLevel TF-IDF:  {0}\".format(accuracy))\n",
    "\n",
    "# Xgboost with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(xgboost.XGBClassifier(), xtrainTFIDFngram.tocsc(), trainY, xtestTFIDFngram.tocsc())\n",
    "print (\"Xgboost , N-Gram Vectors:  {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.7 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN , Count Vectors:  0.63\n",
      "KNN , WordLevel TF-IDF:  0.702\n",
      "KNN , N-Gram Vectors:  0.744\n"
     ]
    }
   ],
   "source": [
    "# Define k\n",
    "k = 9\n",
    "\n",
    "#KNN with Count Vectors\n",
    "accuracy = trainModel(KNeighborsClassifier(k), xtraincount, trainY, xtestcount)\n",
    "print (\"KNN , Count Vectors:  {0}\".format(accuracy))\n",
    "\n",
    "# KNN with WordLevel TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(KNeighborsClassifier(k), xtrainTFIDF, trainY, xtestTFIDF)\n",
    "print (\"KNN , WordLevel TF-IDF:  {0}\".format(accuracy))\n",
    "\n",
    "# KNN with N-Gram TF-IDF vectors as feature vector\n",
    "accuracy = trainModel(KNeighborsClassifier(k), xtrainTFIDFngram, trainY, xtestTFIDFngram)\n",
    "print (\"KNN , N-Gram Vectors:  {0}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
